---
layout: post
title: Capstone project 
author: [Richard Kuo]
category: [Lecture]
tags: [jekyll, ai]
---

Capstone project æœŸæœ«å°ˆé¡Œå¯¦ä½œï¼šè©¦ç©¿è¡£æœ (Virtual try-on)

---
##  Virtual try-on(è©¦ç©¿è¡£æœ)


### ç³»çµ±ç°¡ä»‹åŠåŠŸèƒ½èªªæ˜

1. **ç³»çµ±ç°¡ä»‹**ï¼š


2. **åŠŸèƒ½èªªæ˜**ï¼š

---
### ç³»çµ±æ–¹å¡Šåœ–
![]()
AIæ¨¡å‹èªªæ˜

---
### è£½ä½œæ­¥é©Ÿ

1.å»ºç«‹è³‡æ–™é›†dataset
2.ç§»æ¤ç¨‹å¼ to kaggle
3.kaggleä¸Šè¨“ç·´æ¨¡å‹
4.kaggleä¸Šæ¸¬è©¦æ¨¡å‹

---
### ç³»çµ±æ¸¬è©¦åŠæˆæœå±•ç¤º
<iframe width="853" height="480" src="https://www.youtube.com/embed/oWdbG_mnx7w" title="å»éœæ ¼è¯æ»‹å­¸é™¢ç•¶æœ€æƒ¡å­¸å§Šçš„ä¸€å¤©ğŸï¼" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br>
<br>


**å°ˆé¡Œå¯¦ä½œæ­¥é©Ÿ:**
1. å»ºç«‹èº«é«”å‹•ä½œä¹‹å§¿æ…‹ç…§ç‰‡è³‡æ–™é›† (ä¾‹å¦‚ï¼š5 poses , take 20 pictures of each pose)<br>
2. å§‹ç”¨**MMPose** è¾¨è­˜å‡ºç…§ç‰‡ä¸­çš„å„å§¿å‹¢ä¹‹èº«é«”é—œéµé» (use MMPose convert 16 keypoints (x,y) of each pose)<br>
3. ç”¢ç”Ÿå§¿æ…‹é—œéµé»è³‡æ–™é›† x_train.append(pose_keypoints) ( x_train.shape = (20x5, 16, 2), y_train.shape= (20x5, 1) )<br>
4. å»ºç«‹DNNæ¨¡å‹ä¸¦è¨“ç·´æ¨¡å‹, ç„¶å¾Œä¸‹è¼‰æ¨¡å‹æª”`pose_dnn.h5`è‡³PC <br>
5. æ–¼PCå»ºç«‹å¸¶cameraè¼¸å…¥ä¹‹æœå‹™å™¨ç¨‹å¼, è¼‰å…¥æ¨¡å‹`pose_dnn.h5`é€²è¡Œå§¿æ…‹å‹•ä½œè¾¨è­˜ <br>

**æ¨¡å‹å»ºæ§‹èˆ‡è¨“ç·´ä¹‹ç¨‹å¼æ¨£æœ¬** (PC or Kaggle)<br>

```
input_shape=(16,2)
num_classes=5

inputs = layers.Input(shape=input_shape)
x = layers.Dense(128)(inputs)
outputs = layers.Dense(num_classes, activation="softmax")(x)
model = models.Model(inputs=inputs, outputs=outputs)

models.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])

history = model.fit(x_train, y_train, batch_size=1, epochs=20, validation_data=(x_test, y_test))
models.save_model(model, 'pose_dnn.h5')
```

**å§¿æ…‹è¾¨è­˜æœå‹™å™¨ä¹‹ç¨‹å¼æ¨£æœ¬** (PC with Camera)<br>

```
model = models.load_model('models/pose_dnn.h5')
labels = ['stand', 'raise-right-arm', 'raise-left-arm', 'cross arms','both-arms-left']

cap = cv2.VideoCapture(0)

while(cap.isOpened()):
    ret, frame = cap.read()
    image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)
    
    mmdet_results = inference_detector(det_model, image) # äººç‰©åµæ¸¬ç”¢ç”ŸBBox
    person_results = process_mmdet_results(mmdet_results, args.det_cat_id) # è¨˜ä½äººç‰©ä¹‹BBox  
    pose_results, returned_outputs = inference_top_down_pose_model(...) # æ„Ÿæ¸¬å§¿æ…‹ç”¢ç”Ÿpose keypoints
    
    x_test = np.array(preson_results).reshape(1,16,2) # å°‡Keypoints List è½‰æˆ numpy Array
    preds = model.fit(x_test) # è¾¨è­˜å§¿æ…‹å‹•ä½œ
    maxindex = int(np.argmax(preds))
    txt = labels[maxindex]
    print(txt)
```



---
### Hand Pose
![](https://github.com/facebookresearch/InterHand2.6M/blob/main/assets/teaser.gif?raw=true)
**Dataset:** [InterHand2.6M](https://github.com/facebookresearch/InterHand2.6M)<br>

1. Download pre-trained InterNet from [here](https://drive.google.com/drive/folders/1BET1f5p2-1OBOz6aNLuPBAVs_9NLz5Jo?usp=sharing)
2. Put the model at `demo` folder
3. Go to `demo` folder and edit `bbox` in [here](https://github.com/facebookresearch/InterHand2.6M/blob/5de679e614151ccfd140f0f20cc08a5f94d4b147/demo/demo.py#L74)
4. run `python demo.py --gpu 0 --test_epoch 20`
5. You can see `result_2D.jpg` and 3D viewer.

**Camera positios visualization demo**
1. `cd tool/camera_visualize`
2. Run `python camera_visualize.py`


<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*

